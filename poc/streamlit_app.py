"""
Streamlit web interface for the AI Research Project - LangChain/LangGraph RAG System.
"""

import streamlit as st
import logging
import traceback
from datetime import datetime
from typing import Optional, List, Dict, Any
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd

from src.models.question import Question, QuestionType
from src.models.document import DocumentChunk
from src.services.vector_store import VectorStore
from src.services.embedding_service import EmbeddingService
from src.langchain.services.langchain_answer_service import LangChainAnswerService
from src.utils.config import get_config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page config
st.set_page_config(
    page_title="AI Research Project - LangChain/LangGraph RAG System",
    page_icon="ï¿½ï¿½",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #4CAF50, #2196F3);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def initialize_services():
    """Initialize all required services (cached)."""
    try:
        # Load configuration
        config = get_config()
        logger.info("Configuration loaded successfully")
        
        # Initialize embedding service
        embedding_service = EmbeddingService(
            api_key=config.openai_api_key,
            model=config.openai_embedding_model,
            max_retries=config.openai_max_retries,
            retry_delay=config.openai_retry_delay
        )
        
        # Initialize vector store
        vector_store = VectorStore(
            host=config.milvus_host,
            token=config.milvus_token,
            collection_name=config.milvus_collection_name
        )
        
        # Initialize LangChain answer service
        langchain_service = LangChainAnswerService(
            vector_store=vector_store,
            embedding_service=embedding_service,
            openai_api_key=config.openai_api_key,
            vector_db_threshold=0.7,
            web_search_threshold=0.6,
            llm_direct_threshold=0.5
        )
        
        return config, embedding_service, vector_store, langchain_service
        
    except Exception as e:
        st.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error(f"Service initialization failed: {e}")
        return None, None, None, None


def create_sample_chunks() -> List[DocumentChunk]:
    """Create sample document chunks for testing."""
    sample_chunks = [
        DocumentChunk(
            id="chunk_1",
            content="LangChainì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì²´ì¸, ì—ì´ì „íŠ¸, ë©”ëª¨ë¦¬ ë“±ì˜ êµ¬ì„± ìš”ì†Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            metadata={"source": "langchain_guide.pdf", "page": 1, "type": "definition"}
        ),
        DocumentChunk(
            id="chunk_2", 
            content="LangGraphëŠ” LangChainì˜ í™•ì¥ìœ¼ë¡œ, ë³µì¡í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ê·¸ë˜í”„ í˜•íƒœë¡œ êµ¬ì„±í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. ìƒíƒœ ê´€ë¦¬ì™€ ì¡°ê±´ë¶€ ì‹¤í–‰ì„ ì§€ì›í•©ë‹ˆë‹¤.",
            metadata={"source": "langgraph_tutorial.pdf", "page": 1, "type": "definition"}
        ),
        DocumentChunk(
            id="chunk_3",
            content="ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” AIì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµìœ¼ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.",
            metadata={"source": "ml_basics.pdf", "page": 3, "type": "concept"}
        ),
        DocumentChunk(
            id="chunk_4",
            content="ë”¥ëŸ¬ë‹ì€ ì¸ê³µì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤. ë‹¤ì¸µ ì‹ ê²½ë§ì„ í†µí•´ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            metadata={"source": "deep_learning.pdf", "page": 2, "type": "concept"}
        ),
        DocumentChunk(
            id="chunk_5",
            content="RAG(Retrieval-Augmented Generation)ëŠ” ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ LLMì˜ ë‹µë³€ ìƒì„±ì„ ë³´ê°•í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.",
            metadata={"source": "rag_paper.pdf", "page": 1, "type": "technique"}
        )
    ]
    return sample_chunks


def display_individual_answer(source_name: str, source_data: Dict[str, Any]):
    """Display individual source answer."""
    if not source_data or not source_data.get('success', False):
        st.warning(f"âŒ {source_name} ì†ŒìŠ¤ì—ì„œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return
    
    answer = source_data.get('answer')
    if answer:
        st.markdown(answer.content)
        
        # Show confidence metrics
        confidence = answer.confidence
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ê´€ë ¨ì„±", f"{confidence.relevance:.2f}")
        with col2:
            st.metric("ì™„ì„±ë„", f"{confidence.completeness:.2f}")
        with col3:
            st.metric("ì •í™•ë„", f"{confidence.accuracy:.2f}")
        with col4:
            st.metric("ì‹ ë¢°ë„", f"{confidence.reliability:.2f}")
        
        # Show processing time
        processing_time = source_data.get('processing_time', 0)
        st.caption(f"â±ï¸ ì²˜ë¦¬ ì‹œê°„: {processing_time:.0f}ms")
        
        # Show raw result in expander
        with st.expander("ğŸ” ì›ë³¸ ë°ì´í„°"):
            st.json(source_data.get('raw_result', {}))


def main():
    """Main Streamlit application."""
    
    # Header
    st.markdown('<h1 class="main-header">ğŸš€ LangChain/LangGraph RAG System</h1>', unsafe_allow_html=True)
    
    # Initialize services
    config, embedding_service, vector_store, langchain_service = initialize_services()
    
    if not all([config, embedding_service, vector_store, langchain_service]):
        st.error("âš ï¸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # Sidebar
    with st.sidebar:
        st.header("ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # System status
        st.subheader("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ")
        
        try:
            # Vector store stats
            stats = vector_store.get_collection_stats()
            st.metric("ë¬¸ì„œ ìˆ˜", stats.get('total_documents', 0))
            st.metric("ë²¡í„° ì°¨ì›", stats.get('dimension', 'N/A'))
            
        except Exception as e:
            st.warning(f"í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # Service stats
        service_stats = langchain_service.get_service_stats()
        st.subheader("ğŸ“ˆ ì„œë¹„ìŠ¤ í†µê³„")
        st.metric("ì²˜ë¦¬ëœ ì§ˆë¬¸", service_stats['stats']['total_questions'])
        st.metric("ì„±ê³µë¥ ", f"{service_stats['stats']['successful_answers'] / max(1, service_stats['stats']['total_questions']) * 100:.1f}%")
        st.metric("í‰ê·  ì²˜ë¦¬ì‹œê°„", f"{service_stats['stats']['average_processing_time']:.0f}ms")
    
    # Main tabs
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸš€ ì§ˆì˜ì‘ë‹µ", 
        "ğŸ“š ë°ì´í„° ê´€ë¦¬", 
        "ğŸ“Š ë¶„ì„", 
        "â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´"
    ])
    
    with tab1:
        st.header("ğŸš€ LangChain/LangGraph ì§ˆì˜ì‘ë‹µ")
        
        st.info("""
        **LangChain/LangGraph ê¸°ë°˜ ì§€ëŠ¥í˜• ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ**
        
        ì´ ì‹œìŠ¤í…œì€ ì§ˆë¬¸ ìœ í˜•ì„ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ê³ , 
        í•„ìš”ì‹œ ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ì¡°í•©í•˜ì—¬ í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
        """)
        
        # Question input
        question_input = st.text_area(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: LangChainê³¼ LangGraphì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ê³ , ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì¶”ì²œí•´ì£¼ì„¸ìš”",
            height=120
        )
        
        # Answer options
        col_opt1, col_opt2 = st.columns(2)
        with col_opt1:
            show_all_sources = st.checkbox("ğŸ”„ ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‹µë³€ ë°›ê¸°", value=False, 
                                         help="ì²´í¬í•˜ë©´ ë¼ìš°íŒ…ê³¼ ê´€ê³„ì—†ì´ ëª¨ë“  ì†ŒìŠ¤(ë²¡í„°DB, ì›¹ê²€ìƒ‰, LLM)ì—ì„œ ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤.")
        with col_opt2:
            force_hybrid = st.checkbox("ğŸ”€ í•˜ì´ë¸Œë¦¬ë“œ í†µí•© ê°•ì œ", value=False,
                                     help="ì²´í¬í•˜ë©´ ê°€ëŠ¥í•œ ëª¨ë“  ì†ŒìŠ¤ë¥¼ í†µí•©í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            if st.button("ğŸš€ ë‹µë³€ ë°›ê¸°", type="primary"):
                if question_input.strip():
                    with st.spinner("LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘..."):
                        try:
                            # Create question object
                            question = Question(content=question_input.strip())
                            
                            # Get comprehensive answer with options
                            result = langchain_service.get_comprehensive_answer(
                                question, 
                                show_all_sources=show_all_sources,
                                force_hybrid=force_hybrid
                            )
                            
                            if result.get('success'):
                                    st.success("âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                                    
                                    # Display final answer
                                    final_answer = result.get('final_answer')
                                    individual_answers = result.get('individual_answers', {})
                                    
                                    if final_answer:
                                        st.subheader("ğŸ¯ ìµœì¢… í†µí•© ë‹µë³€")
                                        st.markdown(final_answer.content)
                                        
                                        # Show individual source answers if available
                                        if individual_answers:
                                            st.subheader("ğŸ“‹ ì†ŒìŠ¤ë³„ ê°œë³„ ë‹µë³€")
                                            
                                            # Create tabs for each source
                                            source_names = list(individual_answers.keys())
                                            source_display_names = {
                                                'vector_db': 'ğŸ“š ë²¡í„° DB',
                                                'web_search': 'ğŸŒ ì›¹ ê²€ìƒ‰', 
                                                'llm_direct': 'ğŸ¤– LLM ì§ì ‘'
                                            }
                                            
                                            if len(source_names) > 1:
                                                source_tabs = st.tabs([
                                                    source_display_names.get(name, name) 
                                                    for name in source_names
                                                ])
                                                
                                                for i, (source_name, source_data) in enumerate(individual_answers.items()):
                                                    with source_tabs[i]:
                                                        display_individual_answer(source_name, source_data)
                                            else:
                                                # Single source - display directly
                                                for source_name, source_data in individual_answers.items():
                                                    st.write(f"**{source_display_names.get(source_name, source_name)}**")
                                                    display_individual_answer(source_name, source_data)
                                    
                                    # Show routing info
                                    st.subheader("ğŸ§  ë¼ìš°íŒ… ì •ë³´")
                                    routing_info = result.get('routing_info', {})
                                    
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.metric("ë¼ìš°íŒ… ì „ëµ", routing_info.get('strategy', 'N/A'))
                                    with col2:
                                        st.metric("ì§ˆë¬¸ ìœ í˜•", str(routing_info.get('question_type', 'N/A')))
                                    with col3:
                                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{routing_info.get('total_processing_time', 0):.0f}ms")
                                    
                                    # Show confidence scores
                                    confidences = routing_info.get('source_confidences', {})
                                    if confidences:
                                        st.subheader("ğŸ“Š ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„")
                                        
                                        # Create confidence chart
                                        sources = list(confidences.keys())
                                        scores = [confidences[s] * 100 for s in sources]
                                        
                                        fig = go.Figure(data=[
                                            go.Bar(
                                                x=sources,
                                                y=scores,
                                                marker_color=['#4CAF50', '#2196F3', '#FF9800'][:len(sources)],
                                                text=[f"{s:.1f}%" for s in scores],
                                                textposition='auto'
                                            )
                                        ])
                                        fig.update_layout(
                                            title="ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„ ì ìˆ˜",
                                            yaxis_title="ì‹ ë¢°ë„ (%)",
                                            xaxis_title="ë‹µë³€ ì†ŒìŠ¤",
                                            height=400,
                                            showlegend=False
                                        )
                                        st.plotly_chart(fig, use_container_width=True)
                                    
                                    # Show performance metrics
                                    st.subheader("âš¡ ì„±ëŠ¥ ì§€í‘œ")
                                    perf_metrics = result.get('performance_metrics', {})
                                    agent_times = perf_metrics.get('agent_times', {})
                                    
                                    if agent_times:
                                        df = pd.DataFrame([
                                            {"ì—ì´ì „íŠ¸": k.replace('_', ' ').title(), "ì²˜ë¦¬ì‹œê°„ (ms)": v}
                                            for k, v in agent_times.items()
                                        ])
                                        st.dataframe(df, use_container_width=True)
                                    
                                    # Show hybrid comparison if multiple sources
                                    if individual_answers and len(individual_answers) > 1:
                                        st.subheader("âš–ï¸ ë‹µë³€ ë¹„êµ ë¶„ì„")
                                        
                                        # Create comparison table
                                        comparison_data = []
                                        for source_name, source_data in individual_answers.items():
                                            if source_data.get('success'):
                                                answer = source_data.get('answer')
                                                if answer:
                                                    comparison_data.append({
                                                        "ì†ŒìŠ¤": source_display_names.get(source_name, source_name),
                                                        "ë‹µë³€ ê¸¸ì´": f"{len(answer.content)}ì",
                                                        "ì‹ ë¢°ë„": f"{answer.confidence.overall_score():.2f}",
                                                        "ì²˜ë¦¬ì‹œê°„": f"{source_data.get('processing_time', 0):.0f}ms",
                                                        "ì†ŒìŠ¤ íƒ€ì…": answer.primary_source.value
                                                    })
                                        
                                        if comparison_data:
                                            df = pd.DataFrame(comparison_data)
                                            st.dataframe(df, use_container_width=True)
                                            
                                            # Show integration strategy
                                            integration_strategy = final_answer.metadata.get('integration_strategy', 'N/A')
                                            st.info(f"ğŸ”„ **í†µí•© ì „ëµ**: {integration_strategy}")
                                    
                                    # Show metadata
                                    with st.expander("ğŸ” ìƒì„¸ ë©”íƒ€ë°ì´í„°"):
                                        st.json(result.get('metadata', {}))
                                        
                            else:
                                st.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                                
                        except Exception as e:
                            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            logger.error(f"LangGraph error: {e}")
                            with st.expander("ì˜¤ë¥˜ ìƒì„¸ ì •ë³´"):
                                st.code(traceback.format_exc())
                else:
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        
        with col2:
            if st.button("ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„"):
                stats = langchain_service.get_service_stats()
                st.json(stats)
        
        with col3:
            if st.button("ğŸ”„ í†µê³„ ë¦¬ì…‹"):
                langchain_service.reset_stats()
                st.success("í†µê³„ê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
    
    with tab2:
        st.header("ğŸ“š ë°ì´í„° ê´€ë¦¬")
        
        st.subheader("ğŸ“„ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€")
        
        if st.button("ğŸ”„ ìƒ˜í”Œ ë¬¸ì„œ ì¶”ê°€"):
            try:
                with st.spinner("ìƒ˜í”Œ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì¶”ê°€í•˜ëŠ” ì¤‘..."):
                    sample_chunks = create_sample_chunks()
                    
                    # Generate embeddings and store
                    for chunk in sample_chunks:
                        embedding = embedding_service.get_embedding(chunk.content)
                        vector_store.add_document(chunk, embedding)
                    
                    st.success(f"âœ… {len(sample_chunks)}ê°œì˜ ìƒ˜í”Œ ë¬¸ì„œê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
                    
            except Exception as e:
                st.error(f"âŒ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
        
        # Collection management
        st.subheader("ğŸ—‚ï¸ ì»¬ë ‰ì…˜ ê´€ë¦¬")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“Š ì»¬ë ‰ì…˜ í†µê³„"):
                try:
                    stats = vector_store.get_collection_stats()
                    st.json(stats)
                except Exception as e:
                    st.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        with col2:
            if st.button("ğŸ—‘ï¸ ì»¬ë ‰ì…˜ ì´ˆê¸°í™”", type="secondary"):
                if st.session_state.get('confirm_reset'):
                    try:
                        vector_store.reset_collection()
                        st.success("ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.session_state.confirm_reset = False
                        st.rerun()
                    except Exception as e:
                        st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                else:
                    st.warning("âš ï¸ ì´ ì‘ì—…ì€ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.")
                    if st.button("í™•ì¸"):
                        st.session_state.confirm_reset = True
                        st.rerun()
    
    with tab3:
        st.header("ğŸ“Š ì‹œìŠ¤í…œ ë¶„ì„")
        
        # Service statistics
        st.subheader("ğŸ“ˆ ì„œë¹„ìŠ¤ ì„±ëŠ¥")
        
        try:
            stats = langchain_service.get_service_stats()
            service_stats = stats['stats']
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ì´ ì§ˆë¬¸ ìˆ˜", service_stats['total_questions'])
            with col2:
                st.metric("ì„±ê³µí•œ ë‹µë³€", service_stats['successful_answers'])
            with col3:
                success_rate = (service_stats['successful_answers'] / max(1, service_stats['total_questions'])) * 100
                st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
            with col4:
                st.metric("í‰ê·  ì²˜ë¦¬ì‹œê°„", f"{service_stats['average_processing_time']:.0f}ms")
            
            # Source usage chart
            st.subheader("ğŸ“Š ì†ŒìŠ¤ ì‚¬ìš© í˜„í™©")
            source_usage = service_stats['source_usage']
            
            if any(source_usage.values()):
                fig = go.Figure(data=[
                    go.Pie(
                        labels=list(source_usage.keys()),
                        values=list(source_usage.values()),
                        hole=0.4
                    )
                ])
                fig.update_layout(
                    title="ë‹µë³€ ì†ŒìŠ¤ë³„ ì‚¬ìš© ë¹„ìœ¨",
                    height=400
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("ì•„ì§ ì²˜ë¦¬ëœ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            st.error(f"í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    with tab4:
        st.header("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
        
        st.subheader("ğŸ—ï¸ ì•„í‚¤í…ì²˜")
        st.info("""
        **LangChain/LangGraph RAG ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ:**
        
        1. **LangChain Tools**: VectorSearchTool, WebSearchTool, LLMDirectTool
        2. **LangGraph ì›Œí¬í”Œë¡œìš°**: ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ ì‹¤í–‰
        3. **ë²¡í„° ì €ì¥ì†Œ**: Milvus í´ë¼ìš°ë“œ (Zilliz) ì‚¬ìš©
        4. **ì„ë² ë”© ì„œë¹„ìŠ¤**: OpenAI text-embedding-ada-002 ëª¨ë¸
        5. **LLM**: OpenAI GPT-4o-mini ëª¨ë¸
        """)
        
        st.subheader("ğŸš€ ì£¼ìš” íŠ¹ì§•")
        st.success("""
        **LangGraph ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ëŠ¥:**
        
        âœ… **ì§€ëŠ¥ì  ì›Œí¬í”Œë¡œìš°**: ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ìë™ ë¼ìš°íŒ…  
        âœ… **ìƒíƒœ ê´€ë¦¬**: TypedDict ê¸°ë°˜ ìƒíƒœ ì¶”ì   
        âœ… **ì¡°ê±´ë¶€ ì‹¤í–‰**: í•„ìš”í•œ ì—ì´ì „íŠ¸ë§Œ ì„ íƒì  ì‹¤í–‰  
        âœ… **ë³‘ë ¬ ì²˜ë¦¬**: ì—¬ëŸ¬ ì†ŒìŠ¤ ë™ì‹œ ì¡°íšŒ ê°€ëŠ¥  
        âœ… **í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€**: ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•© ë‹µë³€  
        âœ… **ë‚´ì¥ ëª¨ë‹ˆí„°ë§**: ì‹¤í–‰ ì¶”ì  ë° ì„±ëŠ¥ ë¶„ì„  
        âœ… **í™•ì¥ì„±**: ìƒˆë¡œìš´ ë…¸ë“œ/ì—ì´ì „íŠ¸ ì‰½ê²Œ ì¶”ê°€  
        âœ… **í‘œì¤€í™”**: ì—…ê³„ í‘œì¤€ í”„ë ˆì„ì›Œí¬ ì‚¬ìš©  
        """)
        
        st.subheader("ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ")
        tech_stack = {
            "í”„ë ˆì„ì›Œí¬": ["LangChain", "LangGraph", "Streamlit"],
            "AI/ML": ["OpenAI GPT-4o-mini", "OpenAI Embeddings", "Milvus Vector DB"],
            "ì–¸ì–´/ë¼ì´ë¸ŒëŸ¬ë¦¬": ["Python 3.13", "Pydantic", "Poetry"],
            "ì‹œê°í™”": ["Plotly", "Pandas"]
        }
        
        for category, technologies in tech_stack.items():
            st.write(f"**{category}**: {', '.join(technologies)}")
        
        st.subheader("ğŸ“ ì‚¬ìš©ë²•")
        st.markdown("""
        1. **í™˜ê²½ ì„¤ì •**: `.env` íŒŒì¼ì— OpenAI API í‚¤ì™€ Milvus ì •ë³´ ì„¤ì •
        2. **ìƒ˜í”Œ ë°ì´í„°**: 'ğŸ“š ë°ì´í„° ê´€ë¦¬' íƒ­ì—ì„œ ìƒ˜í”Œ ë¬¸ì„œ ì¶”ê°€
        3. **ì§ˆë¬¸ ì…ë ¥**: 'ğŸš€ ì§ˆì˜ì‘ë‹µ' íƒ­ì—ì„œ ìì—°ì–´ ì§ˆë¬¸ ì‘ì„±
        4. **ê²°ê³¼ ë¶„ì„**: ìƒì„±ëœ ë‹µë³€ê³¼ ë¼ìš°íŒ… ì •ë³´ í™•ì¸
        5. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**: 'ğŸ“Š ë¶„ì„' íƒ­ì—ì„œ ì‹œìŠ¤í…œ ì„±ëŠ¥ ì¶”ì 
        """)
        
        # System configuration
        with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì • ì •ë³´"):
            system_config = langchain_service.get_service_stats()
            st.json(system_config)


if __name__ == "__main__":
    main()
