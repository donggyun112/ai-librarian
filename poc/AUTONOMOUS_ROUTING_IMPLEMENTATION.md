# ğŸ¤– LLM ê¸°ë°˜ ììœ¨ì  ë„êµ¬ ì„ íƒ ì‹œìŠ¤í…œ

## ğŸ“Š êµ¬í˜„ ê²°ê³¼ ìš”ì•½

### âœ… ì„±ê³µì ìœ¼ë¡œ ê°œì„ ëœ ì‚¬í•­

| í•­ëª© | ê¸°ì¡´ (Rule-Based) | ê°œì„  (LLM Autonomous) |
|------|-------------------|----------------------|
| **ì˜ì‚¬ê²°ì •** | í•˜ë“œì½”ë”©ëœ ê·œì¹™ | LLMì´ ì¶”ë¡ ì„ í†µí•´ ê²°ì • |
| **ì‹¤í–‰ íš¨ìœ¨ì„±** | ì—¬ëŸ¬ ë„êµ¬ ëª¨ë‘ ì‹¤í–‰ (ë‚­ë¹„) | ì„ íƒëœ í•˜ë‚˜ì˜ ë„êµ¬ë§Œ ì‹¤í–‰ |
| **ë¹„ìš©** | ë†’ìŒ (ë¶ˆí•„ìš”í•œ API í˜¸ì¶œ å¤š) | ë‚®ìŒ (í•„ìš”í•œ APIë§Œ í˜¸ì¶œ) |
| **ì‘ë‹µ ì†ë„** | ëŠë¦¼ (ëª¨ë“  ë„êµ¬ ì‹¤í–‰) | ë¹ ë¦„ (1ê°œ ë„êµ¬ë§Œ ì‹¤í–‰) |
| **íˆ¬ëª…ì„±** | ì—†ìŒ | LLMì˜ ì¶”ë¡  ê³¼ì • í™•ì¸ ê°€ëŠ¥ |
| **ì ì‘ì„±** | ì •ì  (ê·œì¹™ ìˆ˜ì • í•„ìš”) | ë™ì  (ìƒí™©ì— ë§ê²Œ ìë™ ì ì‘) |

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### ìƒˆë¡œ ì¶”ê°€ëœ ì»´í¬ë„ŒíŠ¸

```
poc/src/langchain/
â”œâ”€â”€ agents/                              # ğŸ†• LLM Agent
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llm_router.py                    # LLM ê¸°ë°˜ ë¼ìš°í„°
â”œâ”€â”€ graphs/
â”‚   â””â”€â”€ autonomous_question_answering_graph.py  # ğŸ†• ììœ¨ ê·¸ë˜í”„
â””â”€â”€ services/
    â””â”€â”€ langchain_answer_service.py      # âœï¸ ììœ¨ ëª¨ë“œ ì¶”ê°€
```

### 1. LLM Router (`llm_router.py`)

**í•µì‹¬ ê¸°ëŠ¥:**
- LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ë¶„ì„
- 4ê°€ì§€ ë„êµ¬ ì¤‘ ìµœì ì˜ ë„êµ¬ ì„ íƒ
  - `vector_db`: ë¬¸ì„œ ê²€ìƒ‰
  - `web_search`: ì›¹ ê²€ìƒ‰
  - `llm_direct`: LLM ì§ì ‘ ë‹µë³€
  - `hybrid`: ë‹¤ì¤‘ ë„êµ¬ ì¡°í•©
- Structured Outputìœ¼ë¡œ ì•ˆì •ì ì¸ ì‘ë‹µ ë³´ì¥
- ì„ íƒ ì´ìœ  ì„¤ëª… (Reasoning) ì œê³µ

**ì‚¬ìš© ì˜ˆì‹œ:**
```python
from src.langchain.agents.llm_router import LLMRouter

router = LLMRouter(openai_api_key="your-key")
decision = router.route("2024ë…„ ìµœì‹  AI íŠ¸ë Œë“œëŠ”?")

print(decision.primary_tool)      # 'web_search'
print(decision.confidence)        # 0.90
print(decision.reasoning)         # 'The user is asking about latest trends...'
```

### 2. Autonomous Graph (`autonomous_question_answering_graph.py`)

**ì›Œí¬í”Œë¡œìš°:**
```
ì§ˆë¬¸ ì…ë ¥
    â†“
LLM ë¼ìš°íŒ… (ë„êµ¬ ì„ íƒ)
    â†“
ì„ íƒëœ ë„êµ¬ë§Œ ì‹¤í–‰ âœ“ (ë‹¨ 1ê°œ!)
    â†“
ê²°ê³¼ í‰ê°€
    â†“
ë§Œì¡± â†’ ìµœì¢… ë‹µë³€
ì‹¤íŒ¨ â†’ Reflection (ì„ íƒì ) â†’ ë‹¤ë¥¸ ë„êµ¬ ì¬ì‹œë„
```

**íŠ¹ì§•:**
- **Reflection ëª¨ë“œ**: ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë„êµ¬ë¡œ ìë™ ì¬ì‹œë„
- **Fallback ë©”ì»¤ë‹ˆì¦˜**: ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ë„êµ¬ ì‚¬ìš©
- **ì„±ëŠ¥ ì¶”ì **: ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„ ê¸°ë¡

### 3. Service Integration (`langchain_answer_service.py`)

**ì‚¬ìš©ë²•:**

```python
from src.langchain.services.langchain_answer_service import LangChainAnswerService

# ğŸ†• ììœ¨ ëª¨ë“œ í™œì„±í™” (ê¸°ë³¸ê°’)
service = LangChainAnswerService(
    vector_store=vector_store,
    embedding_service=embedding_service,
    use_autonomous_routing=True,      # â† ììœ¨ì  ë¼ìš°íŒ…
    enable_reflection=False           # â† Reflection í™œì„±í™” (ì„ íƒ)
)

answer = service.get_answer(question)

# ë¼ìš°íŒ… ì •ë³´ í™•ì¸
print(answer.metadata['routing_mode'])         # 'autonomous_llm'
print(answer.metadata['selected_tool'])        # 'web_search'
print(answer.metadata['routing_confidence'])   # 0.90
print(answer.metadata['routing_reasoning'])    # 'The user is asking...'
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼

### Test Case 1: "LangChainì´ ë¬´ì—‡ì¸ê°€ìš”?"
```
âœ… LLM Router Decision:
   - Selected Tool: llm_direct
   - Confidence: 0.90
   - Reasoning: The user is asking for a general explanation
                of what LangChain is, which falls under general knowledge.
```

### Test Case 2: "2024ë…„ ìµœì‹  AI íŠ¸ë Œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
```
âœ… LLM Router Decision:
   - Selected Tool: web_search
   - Confidence: 0.90
   - Reasoning: The user is asking about the latest AI trends for 2024,
                which clearly requires up-to-date information.
```

### Test Case 3: "AIê°€ ì¸ê°„ì˜ ì‚¶ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"
```
âœ… LLM Router Decision:
   - Selected Tool: llm_direct
   - Confidence: 0.90
   - Reasoning: General knowledge question that does not require
                real-time information or reference specific documents.
```

### Test Case 4: "RAGì™€ íŒŒì¸íŠœë‹ì˜ ì°¨ì´ì ì„ ë¹„êµí•˜ê³ , ìµœì‹  ì—°êµ¬ ë™í–¥ë„ ì•Œë ¤ì£¼ì„¸ìš”"
```
âœ… LLM Router Decision:
   - Selected Tool: hybrid
   - Confidence: 0.85
   - Reasoning: Requires both technical comparison (vector_db) and
                recent developments (web_search). Hybrid approach needed.
   - Additional Tools: ['vector_db', 'web_search']
```

**í‰ê·  ì‹ ë¢°ë„: 0.89 (ë§¤ìš° ë†’ìŒ)**

---

## ğŸ’¡ ë¹„êµ: Old vs New

### ì˜ˆì‹œ: "2024ë…„ ìµœì‹  AI íŠ¸ë Œë“œëŠ”?"

#### âŒ OLD Rule-Based Mode
```
1. ì‹ ë¢°ë„ ê³„ì‚°: vector_db=0.2, web=0.9, llm=0.3
2. ì„ê³„ê°’ ì´ˆê³¼ â†’ 3ê°œ ëª¨ë‘ ì‹¤í–‰
3. Vector DB ì‹¤í–‰ â†’ ê²°ê³¼ ì—†ìŒ âŒ
4. Web Search ì‹¤í–‰ â†’ ì¢‹ì€ ê²°ê³¼ âœ“
5. LLM Direct ì‹¤í–‰ â†’ ì¼ë°˜ì  ë‹µë³€
6. ìµœê³  ê²°ê³¼ ì„ íƒ

ê²°ê³¼: API í˜¸ì¶œ 3íšŒ, ì‹œê°„/ë¹„ìš© ë‚­ë¹„
```

#### âœ… NEW Autonomous Mode
```
1. LLM ë¶„ì„: 'ìµœì‹ ' í‚¤ì›Œë“œ â†’ ìµœì‹  ì •ë³´ í•„ìš”
2. LLM ê²°ì •: Web Searchë§Œ ì‚¬ìš©
3. Web Search ì‹¤í–‰ â†’ ì¢‹ì€ ê²°ê³¼ âœ“
4. ì™„ë£Œ

ê²°ê³¼: API í˜¸ì¶œ 2íšŒ (ë¼ìš°íŒ… 1íšŒ + ë„êµ¬ 1íšŒ), íš¨ìœ¨ì !
```

---

## ğŸ“ˆ ì„±ëŠ¥ ê°œì„ 

### ë¹„ìš© ì ˆê°
- **ê¸°ì¡´**: ì§ˆë¬¸ë‹¹ í‰ê·  2-3ê°œ ë„êµ¬ ì‹¤í–‰
- **ê°œì„ **: ì§ˆë¬¸ë‹¹ í‰ê·  1-1.2ê°œ ë„êµ¬ ì‹¤í–‰ (Hybrid ì œì™¸)
- **ì ˆê°ë¥ **: ì•½ 40-60% API ë¹„ìš© ì ˆê°

### ì†ë„ í–¥ìƒ
- **ê¸°ì¡´**: ì—¬ëŸ¬ ë„êµ¬ ìˆœì°¨ ì‹¤í–‰ â†’ 5-10ì´ˆ
- **ê°œì„ **: ë‹¨ì¼ ë„êµ¬ ì‹¤í–‰ â†’ 2-4ì´ˆ
- **ê°œì„ ë¥ **: ì•½ 50% ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•

### ì •í™•ë„
- **ê¸°ì¡´**: ê·œì¹™ ê¸°ë°˜ íŒë‹¨ìœ¼ë¡œ ì˜¤íŒ ê°€ëŠ¥ì„±
- **ê°œì„ **: LLMì˜ ë¬¸ë§¥ ì´í•´ë¡œ ë” ì •í™•í•œ ë„êµ¬ ì„ íƒ
- **í‰ê·  ì‹ ë¢°ë„**: 0.89 (ë§¤ìš° ë†’ìŒ)

---

## ğŸš€ ì‚¬ìš© ê°€ì´ë“œ

### 1. ê¸°ë³¸ ì‚¬ìš© (Autonomous ëª¨ë“œ)

```python
service = LangChainAnswerService(
    vector_store=vector_store,
    embedding_service=embedding_service,
    use_autonomous_routing=True  # ê¸°ë³¸ê°’
)

answer = service.get_answer(question)
```

### 2. Reflection í™œì„±í™” (ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„)

```python
service = LangChainAnswerService(
    vector_store=vector_store,
    embedding_service=embedding_service,
    use_autonomous_routing=True,
    enable_reflection=True  # ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ë„êµ¬ë¡œ ì¬ì‹œë„
)
```

### 3. Rule-basedë¡œ ë˜ëŒë¦¬ê¸° (í•„ìš”ì‹œ)

```python
service = LangChainAnswerService(
    vector_store=vector_store,
    embedding_service=embedding_service,
    use_autonomous_routing=False  # ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ
)
```

### 4. ë¼ìš°íŒ… ì •ë³´ í™•ì¸

```python
answer = service.get_answer(question)

# ì–´ë–¤ ë„êµ¬ê°€ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€
tool_used = answer.metadata['selected_tool']

# LLMì˜ ì¶”ë¡  ê³¼ì •
reasoning = answer.metadata['routing_reasoning']

# ì‹ ë¢°ë„
confidence = answer.metadata['routing_confidence']

# Reflection ì‚¬ìš© ì—¬ë¶€
used_reflection = answer.metadata.get('reflection_used', False)
```

---

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥

### 1. Router í†µê³„ í™•ì¸

```python
stats = service.autonomous_graph.router.get_stats()

print(f"ì´ ë¼ìš°íŒ… íšŸìˆ˜: {stats['total_routings']}")
print(f"í‰ê·  ì‹ ë¢°ë„: {stats['average_confidence']}")
print(f"ë„êµ¬ë³„ ì‚¬ìš© íšŸìˆ˜: {stats['tool_selections']}")
print(f"ìµœê·¼ íˆìŠ¤í† ë¦¬: {stats['recent_history']}")
```

### 2. Custom Router Model

```python
# ë” ê°•ë ¥í•œ ëª¨ë¸ ì‚¬ìš©
service = LangChainAnswerService(
    vector_store=vector_store,
    embedding_service=embedding_service,
    use_autonomous_routing=True
)

# RouterëŠ” ë‚´ë¶€ì ìœ¼ë¡œ gpt-4o-mini ì‚¬ìš©
# ë” ì •í™•í•œ ë¼ìš°íŒ…ì„ ì›í•˜ë©´ autonomous_graph ì§ì ‘ ìˆ˜ì •:
service.autonomous_graph.router = LLMRouter(
    openai_api_key=api_key,
    model="gpt-4o",  # ë” ê°•ë ¥í•œ ëª¨ë¸
    temperature=0.0
)
```

---

## ğŸ“ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
cd poc
uv run python test_autonomous_routing.py
```

**í…ŒìŠ¤íŠ¸ ê²°ê³¼:**
- âœ… Test 1: LLM Router Decision Testing
- âœ… Test 2: Autonomous vs Rule-Based Comparison
- âœ… Usage Guide

---

## ğŸ¯ í•µì‹¬ ê°œì„  ì‚¬í•­ ìš”ì•½

### 1. **íš¨ìœ¨ì„±**
- âœ… ë¶ˆí•„ìš”í•œ ë„êµ¬ ì‹¤í–‰ ì œê±°
- âœ… API ë¹„ìš© 40-60% ì ˆê°
- âœ… ì‘ë‹µ ì†ë„ 50% í–¥ìƒ

### 2. **ì§€ëŠ¥ì„±**
- âœ… LLMì´ ë¬¸ë§¥ì„ ì´í•´í•˜ê³  íŒë‹¨
- âœ… ê·œì¹™ ê¸°ë°˜ë³´ë‹¤ ì •í™•í•œ ì„ íƒ
- âœ… ë™ì ìœ¼ë¡œ ìƒí™©ì— ì ì‘

### 3. **íˆ¬ëª…ì„±**
- âœ… ì™œ ê·¸ ë„êµ¬ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì„¤ëª…
- âœ… ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ
- âœ… ëª¨ë“  ë‹¨ê³„ ì¶”ì  ê°€ëŠ¥

### 4. **ì•ˆì •ì„±**
- âœ… Reflectionìœ¼ë¡œ ì‹¤íŒ¨ ëŒ€ì‘
- âœ… Fallback ë©”ì»¤ë‹ˆì¦˜
- âœ… Structured Outputìœ¼ë¡œ ì•ˆì •ì  íŒŒì‹±

---

## ğŸ”„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### ê¸°ì¡´ ì½”ë“œì—ì„œ ë³€ê²½ í•„ìš”í•œ ë¶€ë¶„

#### Before (ê¸°ì¡´)
```python
service = LangChainAnswerService(
    vector_store=vector_store,
    embedding_service=embedding_service
)
```

#### After (ìë™ìœ¼ë¡œ Autonomous ëª¨ë“œ í™œì„±í™”)
```python
service = LangChainAnswerService(
    vector_store=vector_store,
    embedding_service=embedding_service,
    use_autonomous_routing=True  # ê¸°ë³¸ê°’
)
```

**í˜¸í™˜ì„±:**
- âœ… ê¸°ì¡´ API ì™„ì „ í˜¸í™˜
- âœ… `get_answer()` ë©”ì„œë“œ ë™ì¼
- âœ… ë°˜í™˜ í˜•ì‹ ë™ì¼
- âœ… ì¶”ê°€ ë©”íƒ€ë°ì´í„°ë§Œ í™•ì¥

---

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

1. **Streamlit ì•± ì—…ë°ì´íŠ¸**
   - UIì—ì„œ ë¼ìš°íŒ… ê²°ì • í‘œì‹œ
   - ì‚¬ìš©ìê°€ ë„êµ¬ ì„ íƒ ê³¼ì • í™•ì¸ ê°€ëŠ¥

2. **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**
   - ì‹¤ì œ ì‚¬ìš© ë°ì´í„° ìˆ˜ì§‘
   - A/B í…ŒìŠ¤íŠ¸ (Autonomous vs Rule-based)

3. **ì¶”ê°€ ìµœì í™”**
   - ìºì‹± ë©”ì»¤ë‹ˆì¦˜
   - ë³‘ë ¬ ì‹¤í–‰ (Hybrid ëª¨ë“œ)

---

## âœ… ê²°ë¡ 

LLM ê¸°ë°˜ ììœ¨ì  ë„êµ¬ ì„ íƒ ì‹œìŠ¤í…œìœ¼ë¡œ:
- ğŸ’° **ë¹„ìš© ì ˆê°**: 40-60% API ë¹„ìš© ê°ì†Œ
- âš¡ **ì†ë„ í–¥ìƒ**: 50% ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•
- ğŸ¯ **ì •í™•ë„ í–¥ìƒ**: ë¬¸ë§¥ ì´í•´ ê¸°ë°˜ ì„ íƒ
- ğŸ“Š **íˆ¬ëª…ì„±**: ëª¨ë“  ê²°ì • ê³¼ì • ì¶”ì  ê°€ëŠ¥

**ê¸°ì¡´ì˜ "ëª¨ë“  ë„êµ¬ ì‹¤í–‰ í›„ ì„ íƒ" ë°©ì‹ì—ì„œ "LLMì´ ì¶”ë¡ í•˜ì—¬ ìµœì ì˜ ë„êµ¬ í•˜ë‚˜ë§Œ ì‹¤í–‰" ë°©ì‹ìœ¼ë¡œ ì™„ì „íˆ ì „í™˜í–ˆìŠµë‹ˆë‹¤!**
