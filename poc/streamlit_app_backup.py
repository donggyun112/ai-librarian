"""
Streamlit web interface for the AI Research Project - PDF RAG System.
"""

import streamlit as st
import logging
import traceback
from datetime import datetime
from typing import Optional, List, Dict, Any
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd

from src.models.question import Question, QuestionType
from src.models.document import DocumentChunk
from src.services.vector_store import VectorStore
from src.services.embedding_service import EmbeddingService
from src.agents.vector_search import VectorSearchAgent
from src.utils.config import get_config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page config
st.set_page_config(
    page_title="AI Research Project - PDF RAG System",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
        background: linear-gradient(90deg, #4CAF50, #2196F3);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
    }
    
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #4CAF50;
    }
    
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #c3e6cb;
    }
    
    .error-message {
        background-color: #f8d7da;
        color: #721c24;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #f5c6cb;
    }
    
    .info-box {
        background-color: #e3f2fd;
        color: #0d47a1;
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #bbdefb;
    }
</style>
""", unsafe_allow_html=True)


@st.cache_resource
def initialize_services():
    """Initialize all services with caching."""
    try:
        config = get_config()
        
        # Initialize services
        embedding_service = EmbeddingService(
            api_key=config.openai_api_key,
            model=config.openai_embedding_model,
            max_retries=config.openai_max_retries,
            retry_delay=config.openai_retry_delay
        )
        
        vector_store = VectorStore(
            host=config.milvus_host,
            token=config.milvus_token,
            collection_name=config.milvus_collection_name
        )
        
        search_agent = VectorSearchAgent(
            vector_store=vector_store,
            embedding_service=embedding_service,
            max_results=config.vector_search_max_results,
            min_similarity_threshold=config.vector_search_similarity_threshold,
            max_context_length=config.vector_search_max_context_length
        )
        
        # Initialize question router
        from src.agents.question_router import QuestionRouter
        question_router = QuestionRouter(
            vector_db_threshold=0.7,
            web_search_threshold=0.6,
            llm_direct_threshold=0.5
        )
        
        # Initialize web search agent
        from src.agents.web_search import WebSearchAgent
        web_search_agent = WebSearchAgent(
            max_results=5,
            search_timeout=10,
            enable_fallback=True
        )
        
        # Initialize LLM direct agent
        from src.agents.llm_direct import LLMDirectAgent
        llm_direct_agent = LLMDirectAgent(
            api_key=config.openai_api_key,
            model="gpt-4o-mini",
            max_tokens=1000,
            temperature=0.7
        )
        
        # Initialize answer service
        from src.services.answer_service import AnswerService
        answer_service = AnswerService(
            question_router=question_router,
            vector_search_agent=search_agent,
            web_search_agent=web_search_agent,
            llm_direct_agent=llm_direct_agent
        )
        
        return config, embedding_service, vector_store, search_agent, question_router, answer_service, web_search_agent, llm_direct_agent
        
    except Exception as e:
        st.error(f"ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error(f"Service initialization failed: {e}")
        return None, None, None, None


def create_sample_chunks() -> List[DocumentChunk]:
    """Create sample document chunks."""
    return [
        DocumentChunk(
            id="chunk_ai_1",
            document_id="ai_handbook",
            content="ì¸ê³µì§€ëŠ¥(AI)ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ì»´í“¨í„° ì‹œìŠ¤í…œì˜ ëŠ¥ë ¥ì…ë‹ˆë‹¤. "
                   "ì´ëŠ” í•™ìŠµ, ì¶”ë¡ , ë¬¸ì œ í•´ê²°, ì§€ê°, ì–¸ì–´ ì´í•´ ë“±ì˜ ì¸ì§€ ê¸°ëŠ¥ì„ í¬í•¨í•˜ë©°, "
                   "ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ë³€í™”ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤. "
                   "AIëŠ” í¬ê²Œ ì•½ì¸ê³µì§€ëŠ¥(Narrow AI)ê³¼ ê°•ì¸ê³µì§€ëŠ¥(General AI)ìœ¼ë¡œ êµ¬ë¶„ë©ë‹ˆë‹¤.",
            page_number=1,
            chunk_index=0,
            keywords=["ì¸ê³µì§€ëŠ¥", "AI", "ì»´í“¨í„° ì‹œìŠ¤í…œ", "í•™ìŠµ", "ì¶”ë¡ ", "ì•½ì¸ê³µì§€ëŠ¥", "ê°•ì¸ê³µì§€ëŠ¥"],
            importance_score=0.9
        ),
        DocumentChunk(
            id="chunk_ml_1",
            document_id="ai_handbook",
            content="ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ í•˜ìœ„ ë¶„ì•¼ë¡œ, ì»´í“¨í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ë˜ì§€ ì•Šê³ ë„ "
                   "ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. "
                   "ì§€ë„í•™ìŠµ, ë¹„ì§€ë„í•™ìŠµ, ê°•í™”í•™ìŠµì˜ ì„¸ ê°€ì§€ ì£¼ìš” ìœ í˜•ì´ ìˆìœ¼ë©°, "
                   "ê°ê° ë‹¤ë¥¸ ë¬¸ì œ í•´ê²° ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.",
            page_number=2,
            chunk_index=1,
            keywords=["ë¨¸ì‹ ëŸ¬ë‹", "ë°ì´í„°", "íŒ¨í„´", "í•™ìŠµ", "ì§€ë„í•™ìŠµ", "ë¹„ì§€ë„í•™ìŠµ", "ê°•í™”í•™ìŠµ"],
            importance_score=0.8
        ),
        DocumentChunk(
            id="chunk_dl_1",
            document_id="ai_handbook",
            content="ë”¥ëŸ¬ë‹ì€ ì¸ê³µ ì‹ ê²½ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. "
                   "ì—¬ëŸ¬ ì¸µì˜ ë‰´ëŸ°ìœ¼ë¡œ êµ¬ì„±ëœ ì‹ ê²½ë§ì„ í†µí•´ ë³µì¡í•œ íŒ¨í„´ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ë©°, "
                   "ì´ë¯¸ì§€ ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬, ìŒì„± ì¸ì‹ ë“±ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. "
                   "CNN, RNN, Transformer ë“± ë‹¤ì–‘í•œ ì•„í‚¤í…ì²˜ê°€ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤.",
            page_number=3,
            chunk_index=2,
            keywords=["ë”¥ëŸ¬ë‹", "ì‹ ê²½ë§", "ë‰´ëŸ°", "CNN", "RNN", "Transformer", "ì´ë¯¸ì§€ ì¸ì‹"],
            importance_score=0.85
        ),
        DocumentChunk(
            id="chunk_nlp_1",
            document_id="ai_handbook",
            content="ìì—°ì–´ ì²˜ë¦¬(NLP)ëŠ” ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” AI ë¶„ì•¼ì…ë‹ˆë‹¤. "
                   "í…ìŠ¤íŠ¸ ë¶„ì„, ê°ì • ë¶„ì„, ê¸°ê³„ ë²ˆì—­, ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ì‘ìš©ì´ ìˆìœ¼ë©°, "
                   "ìµœê·¼ GPT, BERT ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ë“±ì¥ìœ¼ë¡œ ê¸‰ì†í•œ ë°œì „ì„ ì´ë£¨ê³  ìˆìŠµë‹ˆë‹¤.",
            page_number=4,
            chunk_index=3,
            keywords=["ìì—°ì–´ ì²˜ë¦¬", "NLP", "í…ìŠ¤íŠ¸ ë¶„ì„", "ê¸°ê³„ ë²ˆì—­", "GPT", "BERT", "ì–¸ì–´ ëª¨ë¸"],
            importance_score=0.9
        ),
        DocumentChunk(
            id="chunk_cv_1",
            document_id="ai_handbook",
            content="ì»´í“¨í„° ë¹„ì „ì€ ë””ì§€í„¸ ì´ë¯¸ì§€ë‚˜ ë¹„ë””ì˜¤ë¡œë¶€í„° ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” AI ë¶„ì•¼ì…ë‹ˆë‹¤. "
                   "ê°ì²´ íƒì§€, ì´ë¯¸ì§€ ë¶„í• , ì–¼êµ´ ì¸ì‹, ì˜ë£Œ ì˜ìƒ ë¶„ì„ ë“±ì— í™œìš©ë˜ë©°, "
                   "ììœ¨ì£¼í–‰ì°¨, ë³´ì•ˆ ì‹œìŠ¤í…œ, ì˜ë£Œ ì§„ë‹¨ ë“± ì‹¤ìƒí™œì— ê´‘ë²”ìœ„í•˜ê²Œ ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
            page_number=5,
            chunk_index=4,
            keywords=["ì»´í“¨í„° ë¹„ì „", "ì´ë¯¸ì§€", "ê°ì²´ íƒì§€", "ì–¼êµ´ ì¸ì‹", "ììœ¨ì£¼í–‰", "ì˜ë£Œ ì§„ë‹¨"],
            importance_score=0.8
        )
    ]


def main():
    """Main Streamlit application."""
    
    # Header
    st.markdown('<h1 class="main-header">ğŸ¤– AI Research Project</h1>', unsafe_allow_html=True)
    st.markdown('<h3 style="text-align: center; color: #666;">PDF RAG System - Vector Search Demo</h3>', 
               unsafe_allow_html=True)
    
    # Initialize services
    with st.spinner("ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
        services = initialize_services()
        if len(services) == 8:
            config, embedding_service, vector_store, search_agent, question_router, answer_service, web_search_agent, llm_direct_agent = services
        elif len(services) == 6:
            config, embedding_service, vector_store, search_agent, question_router, answer_service = services
            web_search_agent, llm_direct_agent = None, None
        else:
            # Fallback for older version
            config, embedding_service, vector_store, search_agent = services[:4]
            question_router, answer_service, web_search_agent, llm_direct_agent = None, None, None, None
    
    if not all([config, embedding_service, vector_store, search_agent]):
        st.error("ì„œë¹„ìŠ¤ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
    
    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # System health check
        st.subheader("ğŸ¥ ì‹œìŠ¤í…œ ìƒíƒœ")
        if st.button("í—¬ìŠ¤ ì²´í¬"):
            with st.spinner("ìƒíƒœ í™•ì¸ ì¤‘..."):
                health = vector_store.health_check()
                if health:
                    st.success("âœ… ë²¡í„° DB ì—°ê²° ì •ìƒ")
                else:
                    st.error("âŒ ë²¡í„° DB ì—°ê²° ì‹¤íŒ¨")
        
        # Collection stats
        st.subheader("ğŸ“Š ì»¬ë ‰ì…˜ í†µê³„")
        if st.button("í†µê³„ ìƒˆë¡œê³ ì¹¨"):
            with st.spinner("í†µê³„ ë¡œë”© ì¤‘..."):
                stats = vector_store.get_collection_stats()
                st.json(stats)
        
        # Configuration info
        st.subheader("ğŸ”§ ì„¤ì • ì •ë³´")
        with st.expander("ì„ë² ë”© ì„¤ì •"):
            embedding_info = embedding_service.get_embedding_info()
            st.json(embedding_info)
        
        with st.expander("ê²€ìƒ‰ ì„¤ì •"):
            search_stats = search_agent.get_search_stats()
            st.json(search_stats)
    
    # Main content
    tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
        "ğŸ” ì§ˆì˜ì‘ë‹µ", 
        "ğŸ¤– í†µí•© ë‹µë³€", 
        "ğŸš€ LangGraph ë‹µë³€",  # ìƒˆë¡œìš´ íƒ­
        "ğŸ§  ì§ˆë¬¸ ë¼ìš°í„°", 
        "ğŸ“š ë°ì´í„° ê´€ë¦¬", 
        "ğŸ“Š ë¶„ì„", 
        "â„¹ï¸ ì •ë³´"
    ])
    
    with tab1:
        st.header("ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ")
        
        # Question input
        col1, col2 = st.columns([3, 1])
        
        with col1:
            question_text = st.text_area(
                "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="ì˜ˆ: AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                height=100
            )
        
        with col2:
            st.write("ì§ˆë¬¸ ìœ í˜•:")
            question_type = st.selectbox(
                "ìœ í˜• ì„ íƒ",
                ["FACTUAL", "GENERAL", "COMPLEX", "CURRENT_EVENTS"],
                index=0
            )
            
            similarity_threshold = st.slider(
                "ìœ ì‚¬ë„ ì„ê³„ê°’",
                min_value=0.0,
                max_value=1.0,
                value=0.7,
                step=0.05
            )
        
        # Search button
        if st.button("ğŸ” ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±", type="primary"):
            if not question_text.strip():
                st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                    try:
                        # Create question object
                        question = Question(
                            id=f"q_{datetime.now().timestamp()}",
                            content=question_text,
                            question_type=QuestionType(question_type.lower()),
                            keywords=question_text.split()[:5],  # Simple keyword extraction
                            preferred_sources=["vector_db"],
                            context_needed=True,
                            language="ko"
                        )
                        
                        # Check if agent can handle the question
                        can_handle = search_agent.can_handle_question(question)
                        
                        if can_handle:
                            # Search for relevant content
                            search_agent.min_similarity_threshold = similarity_threshold
                            relevant_chunks = search_agent.search_relevant_content(question)
                            
                            # Generate answer
                            answer = search_agent.generate_answer(question)
                            
                            if answer:
                                # Display answer
                                st.success("âœ… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                
                                # Answer content
                                st.subheader("ğŸ“ ë‹µë³€")
                                st.write(answer.content)
                                
                                # Answer metrics
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("ì‹ ë¢°ë„", f"{answer.confidence_score:.2f}")
                                with col2:
                                    st.metric("ì²˜ë¦¬ì‹œê°„", f"{answer.processing_time_ms}ms")
                                with col3:
                                    st.metric("ì†ŒìŠ¤ ìˆ˜", len(answer.sources))
                                with col4:
                                    st.metric("ê´€ë ¨ì„±", f"{answer.relevance_score:.2f}")
                                
                                # Source information
                                if answer.sources:
                                    st.subheader("ğŸ“š ì°¸ì¡° ì†ŒìŠ¤")
                                    for i, source in enumerate(answer.sources, 1):
                                        with st.expander(f"ì†ŒìŠ¤ {i}: {source.title} (ìœ ì‚¬ë„: {source.relevance_score:.3f})"):
                                            st.write(source.snippet)
                                
                                # Search results visualization
                                if relevant_chunks:
                                    st.subheader("ğŸ” ê²€ìƒ‰ ê²°ê³¼")
                                    
                                    # Create DataFrame for visualization
                                    df = pd.DataFrame([
                                        {
                                            "ì²­í¬ ID": chunk["chunk_id"],
                                            "ìœ ì‚¬ë„ ì ìˆ˜": chunk["similarity_score"],
                                            "ê´€ë ¨ì„± ì ìˆ˜": chunk.get("search_relevance", 0),
                                            "ì¤‘ìš”ë„": chunk.get("importance_score", 0),
                                            "í˜ì´ì§€": chunk.get("page_number", "?"),
                                            "ë¯¸ë¦¬ë³´ê¸°": chunk.get("content_preview", "")[:100] + "..."
                                        }
                                        for chunk in relevant_chunks
                                    ])
                                    
                                    st.dataframe(df, use_container_width=True)
                                    
                                    # Similarity score chart
                                    fig = px.bar(
                                        df, 
                                        x="ì²­í¬ ID", 
                                        y="ìœ ì‚¬ë„ ì ìˆ˜",
                                        title="ê²€ìƒ‰ ê²°ê³¼ ìœ ì‚¬ë„ ì ìˆ˜",
                                        color="ìœ ì‚¬ë„ ì ìˆ˜",
                                        color_continuous_scale="viridis"
                                    )
                                    st.plotly_chart(fig, use_container_width=True)
                                
                            else:
                                st.error("ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.warning("ì´ ìœ í˜•ì˜ ì§ˆë¬¸ì€ í˜„ì¬ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            
                    except Exception as e:
                        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        logger.error(f"Question processing error: {e}")
                        st.code(traceback.format_exc())
    
    with tab2:
        st.header("í†µí•© ë‹µë³€ ì‹œìŠ¤í…œ")
        
        if answer_service and question_router and web_search_agent and llm_direct_agent:
            st.info("ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•œ í†µí•© ë‹µë³€ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì†ŒìŠ¤ ì¡°í•©ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.")
            
            # Service status
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("ğŸ” ë²¡í„° ê²€ìƒ‰", "âœ… í™œì„±")
            with col2:
                st.metric("ğŸŒ ì›¹ ê²€ìƒ‰", "âœ… í™œì„±")
            with col3:
                st.metric("ğŸ¤– LLM ì§ì ‘", "âœ… í™œì„±")
            with col4:
                st.metric("ğŸ¯ ë¼ìš°í„°", "âœ… í™œì„±")
                
            # Question input
            st.subheader("ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")
            
            integrated_question = st.text_area(
                "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="ì˜ˆ: 2024ë…„ ìµœì‹  AI ê¸°ìˆ ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                height=120,
                key="integrated_question"
            )
            
            col1, col2 = st.columns([3, 1])
            
            with col1:
                use_hybrid = st.checkbox("í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ê°•ì œ ì‚¬ìš©", help="ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ê°•ì œë¡œ ì¡°í•©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤")
            
            with col2:
                if st.button("ğŸš€ í†µí•© ë‹µë³€ ìƒì„±", type="primary", key="integrated_answer"):
                    if integrated_question.strip():
                        with st.spinner("í†µí•© ë‹µë³€ ìƒì„± ì¤‘..."):
                            try:
                                from src.models.question import Question, QuestionType
                                
                                # Create question
                                question = Question(
                                    id=f"integrated_{datetime.now().timestamp()}",
                                    content=integrated_question,
                                    question_type=QuestionType.UNKNOWN,
                                    language="ko"
                                )
                                
                                # Get integrated answer
                                answer = answer_service.get_answer(question)
                                
                                if answer:
                                    st.success("âœ… í†µí•© ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                    
                                    # Display answer
                                    st.subheader("ğŸ“‹ ë‹µë³€")
                                    st.markdown(answer.content)
                                    
                                    # Answer metadata
                                    st.subheader("ğŸ“Š ë‹µë³€ ì •ë³´")
                                    
                                    col1, col2, col3 = st.columns(3)
                                    
                                    with col1:
                                        st.metric("ì‹ ë¢°ë„", f"{answer.confidence_score:.2f}")
                                        st.metric("ê´€ë ¨ì„±", f"{answer.relevance_score:.2f}")
                                        
                                    with col2:
                                        st.metric("ì™„ì„±ë„", f"{answer.completeness_score:.2f}")
                                        st.metric("ì •í™•ë„", f"{answer.accuracy_score:.2f}")
                                        
                                    with col3:
                                        st.metric("ì²˜ë¦¬ ì‹œê°„", f"{answer.processing_time_ms}ms")
                                        st.metric("í† í° ì‚¬ìš©", f"{int(answer.tokens_used)}")
                                    
                                    # Source information
                                    if answer.sources:
                                        st.subheader("ğŸ“š ì¶œì²˜ ì •ë³´")
                                        
                                        for i, source in enumerate(answer.sources[:5], 1):
                                            with st.expander(f"ğŸ“ ì¶œì²˜ {i}: {source.title}"):
                                                st.write(f"**ìœ í˜•:** {source.source_type}")
                                                st.write(f"**ê´€ë ¨ì„±:** {source.relevance_score:.2f}")
                                                if source.url:
                                                    st.write(f"**ë§í¬:** {source.url}")
                                                if source.excerpt:
                                                    st.write(f"**ë‚´ìš©:** {source.excerpt}")
                                    
                                    # Routing information
                                    if 'routing_strategy' in answer.metadata:
                                        st.subheader("ğŸ¯ ë¼ìš°íŒ… ì •ë³´")
                                        
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            st.write(f"**ì „ëµ:** {answer.metadata['routing_strategy']}")
                                            st.write(f"**ì†ŒìŠ¤ ìˆ˜:** {answer.metadata.get('sources_attempted', 'N/A')}")
                                            st.write(f"**í•˜ì´ë¸Œë¦¬ë“œ:** {'ì˜ˆ' if answer.metadata.get('hybrid_approach', False) else 'ì•„ë‹ˆì˜¤'}")
                                            
                                        with col2:
                                            if 'source_confidences' in answer.metadata:
                                                confidences = answer.metadata['source_confidences']
                                                
                                                import plotly.graph_objects as go
                                                
                                                fig = go.Figure(data=[
                                                    go.Bar(
                                                        x=list(confidences.keys()),
                                                        y=list(confidences.values()),
                                                        marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']
                                                    )
                                                ])
                                                
                                                fig.update_layout(
                                                    title="ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„",
                                                    xaxis_title="ì†ŒìŠ¤",
                                                    yaxis_title="ì‹ ë¢°ë„",
                                                    height=300,
                                                    yaxis=dict(range=[0, 1])
                                                )
                                                
                                                st.plotly_chart(fig, use_container_width=True)
                                    
                                    # Tags
                                    if hasattr(answer, 'tags') and answer.tags:
                                        st.subheader("ğŸ·ï¸ íƒœê·¸")
                                        tag_cols = st.columns(min(len(answer.tags), 4))
                                        for i, tag in enumerate(answer.tags):
                                            with tag_cols[i % 4]:
                                                st.badge(tag)
                                                
                                else:
                                    st.error("ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                                    
                            except Exception as e:
                                st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                                st.code(traceback.format_exc())
                    else:
                        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        
            # Service statistics
            st.subheader("ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„")
            
            try:
                service_stats = answer_service.get_service_stats()
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:**")
                    for agent in service_stats['available_agents']:
                        st.write(f"â€¢ {agent}")
                        
                with col2:
                    st.write("**ì§€ì› ê¸°ëŠ¥:**")
                    st.write(f"â€¢ ë¼ìš°íŒ… í™œì„±í™”: {'ì˜ˆ' if service_stats['routing_enabled'] else 'ì•„ë‹ˆì˜¤'}")
                    st.write(f"â€¢ í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›: {'ì˜ˆ' if service_stats['hybrid_support'] else 'ì•„ë‹ˆì˜¤'}")
                    
            except Exception as e:
                st.error(f"ì„œë¹„ìŠ¤ í†µê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
                
        else:
            st.warning("í†µí•© ë‹µë³€ ì„œë¹„ìŠ¤ê°€ ì™„ì „íˆ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            missing_components = []
            if not answer_service:
                missing_components.append("Answer Service")
            if not question_router:
                missing_components.append("Question Router")
            if not web_search_agent:
                missing_components.append("Web Search Agent")
            if not llm_direct_agent:
                missing_components.append("LLM Direct Agent")
                
            st.error(f"ëˆ„ë½ëœ êµ¬ì„±ìš”ì†Œ: {', '.join(missing_components)}")
    
    with tab3:
        st.header("ì§ˆë¬¸ ë¼ìš°í„° í…ŒìŠ¤íŠ¸")
        
        if question_router:
            st.info("ì§ˆë¬¸ ë¼ìš°í„°ê°€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ìµœì ì˜ ë‹µë³€ ì†ŒìŠ¤ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.")
            
            # Router configuration
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ”§ ë¼ìš°í„° ì„¤ì •")
                router_stats = question_router.get_routing_stats()
                
                st.write("**ì„ê³„ê°’:**")
                for source, threshold in router_stats['thresholds'].items():
                    st.write(f"- {source}: {threshold}")
                    
                st.write("**íŒ¨í„´ ìˆ˜:**")
                for category, count in router_stats['pattern_counts'].items():
                    st.write(f"- {category}: {count}ê°œ")
                    
            with col2:
                st.subheader("ğŸ¯ ì§€ì› ê¸°ëŠ¥")
                st.write("**ì§ˆë¬¸ ìœ í˜•:**")
                for qtype in router_stats['supported_question_types']:
                    st.write(f"- {qtype}")
                    
                st.write("**ë¼ìš°íŒ… ì „ëµ:**")
                for strategy in router_stats['routing_strategies']:
                    st.write(f"- {strategy}")
            
            # Question analysis
            st.subheader("ğŸ§ª ì§ˆë¬¸ ë¶„ì„ í…ŒìŠ¤íŠ¸")
            
            test_question = st.text_area(
                "ë¶„ì„í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="ì˜ˆ: AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                height=100
            )
            
            if st.button("ğŸ” ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…", type="primary"):
                if test_question.strip():
                    with st.spinner("ì§ˆë¬¸ ë¶„ì„ ì¤‘..."):
                        try:
                            from src.models.question import Question, QuestionType
                            
                            # Create question object
                            question = Question(
                                id=f"test_{datetime.now().timestamp()}",
                                content=test_question,
                                question_type=QuestionType.UNKNOWN,
                                language="ko"
                            )
                            
                            # Analyze question
                            analyzed_question = question_router.analyze_question(question)
                            
                            # Route question
                            routing_result = question_router.route_question(analyzed_question)
                            
                            # Display results
                            st.success("âœ… ì§ˆë¬¸ ë¶„ì„ ì™„ë£Œ!")
                            
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.subheader("ğŸ“‹ ì§ˆë¬¸ ë¶„ì„ ê²°ê³¼")
                                st.write(f"**ìœ í˜•:** {analyzed_question.question_type.value}")
                                st.write(f"**ë³µì¡ë„:** {analyzed_question.complexity_score:.2f}")
                                st.write(f"**ì»¨í…ìŠ¤íŠ¸ í•„ìš”:** {analyzed_question.context_needed}")
                                st.write(f"**í‚¤ì›Œë“œ:** {', '.join(analyzed_question.keywords[:5])}")
                                
                            with col2:
                                st.subheader("ğŸ¯ ë¼ìš°íŒ… ê²°ê³¼")
                                st.write(f"**ì „ëµ:** {routing_result['routing_strategy']}")
                                st.write(f"**ì¶”ì²œ ì†ŒìŠ¤:** {', '.join(routing_result['recommended_sources'])}")
                                st.write(f"**í•˜ì´ë¸Œë¦¬ë“œ í•„ìš”:** {routing_result['requires_hybrid']}")
                                st.write(f"**ì²˜ë¦¬ ìš°ì„ ìˆœìœ„:** {routing_result['processing_priority']}")
                                st.write(f"**ì˜ˆìƒ ì²˜ë¦¬ì‹œê°„:** {routing_result['estimated_processing_time']}ms")
                            
                            # Confidence scores chart
                            st.subheader("ğŸ“ˆ ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„")
                            confidences = routing_result['source_confidences']
                            
                            import plotly.graph_objects as go
                            
                            fig = go.Figure(data=[
                                go.Bar(
                                    x=list(confidences.keys()),
                                    y=list(confidences.values()),
                                    marker_color=['#1f77b4', '#ff7f0e', '#2ca02c']
                                )
                            ])
                            
                            fig.update_layout(
                                title="ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„ ì ìˆ˜",
                                xaxis_title="ë‹µë³€ ì†ŒìŠ¤",
                                yaxis_title="ì‹ ë¢°ë„ ì ìˆ˜",
                                yaxis=dict(range=[0, 1])
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Routing explanation
                            st.subheader("ğŸ’¡ ë¼ìš°íŒ… ì„¤ëª…")
                            if routing_result['routing_strategy'] == 'vector_db_only':
                                st.info("ğŸ“š ì´ ì§ˆë¬¸ì€ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì´ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤. ë²¡í„° DBì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                            elif routing_result['routing_strategy'] == 'web_search_only':
                                st.info("ğŸŒ ì´ ì§ˆë¬¸ì€ ìµœì‹  ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ í†µí•´ ë‹µë³€í•©ë‹ˆë‹¤.")
                            elif routing_result['routing_strategy'] == 'llm_direct_only':
                                st.info("ğŸ¤– ì´ ì§ˆë¬¸ì€ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•©ë‹ˆë‹¤. LLMì´ ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤.")
                            elif 'hybrid' in routing_result['routing_strategy']:
                                st.info("ğŸ”„ ì´ ì§ˆë¬¸ì€ ë³µí•©ì ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ì†ŒìŠ¤ë¥¼ ì¡°í•©í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.")
                                
                        except Exception as e:
                            st.error(f"ì§ˆë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                            st.code(traceback.format_exc())
                else:
                    st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    
            # Pre-defined test cases
            st.subheader("ğŸ§ª ë¯¸ë¦¬ ì •ì˜ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
            
            test_cases = {
                "ì‚¬ì‹¤ì  ì§ˆë¬¸": "AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ìµœì‹  ì •ë³´ ì§ˆë¬¸": "2024ë…„ ìµœì‹  AI ê¸°ìˆ  ë™í–¥ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
                "ì¼ë°˜ì  ì§ˆë¬¸": "í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ë ¤ë©´ ì–´ë–»ê²Œ ì‹œì‘í•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œìš”?",
                "ë³µí•©ì  ì§ˆë¬¸": "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ê³ , ê°ê°ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•´ì£¼ì„¸ìš”."
            }
            
            selected_case = st.selectbox("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì„ íƒ:", list(test_cases.keys()))
            
            if st.button("ì„ íƒí•œ ì¼€ì´ìŠ¤ ë¶„ì„"):
                st.text_area("ì„ íƒëœ ì§ˆë¬¸:", test_cases[selected_case], height=100, disabled=True)
                # Trigger analysis with selected case
                test_question = test_cases[selected_case]
                st.rerun()
        else:
            st.warning("ì§ˆë¬¸ ë¼ìš°í„°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            st.info("ì„œë¹„ìŠ¤ë¥¼ ë‹¤ì‹œ ì‹œì‘í•´ì£¼ì„¸ìš”.")
    
    with tab4:
        st.header("ë°ì´í„° ê´€ë¦¬")
        
        # Sample data management
        st.subheader("ğŸ“ ìƒ˜í”Œ ë°ì´í„°")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€"):
                with st.spinner("ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì„ë² ë”© ì¤‘..."):
                    try:
                        # Create sample chunks
                        sample_chunks = create_sample_chunks()
                        
                        # Update word counts
                        for chunk in sample_chunks:
                            chunk.update_counts()
                        
                        # Generate embeddings
                        chunks_with_embeddings = embedding_service.embed_document_chunks(sample_chunks)
                        
                        if chunks_with_embeddings:
                            # Insert into vector store
                            success = vector_store.insert_document_chunks(chunks_with_embeddings)
                            
                            if success:
                                st.success(f"âœ… {len(chunks_with_embeddings)}ê°œì˜ ìƒ˜í”Œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            else:
                                st.error("âŒ ë²¡í„° ì €ì¥ì†Œ ì‚½ì… ì‹¤íŒ¨")
                        else:
                            st.error("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                            
                    except Exception as e:
                        st.error(f"ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€ ì‹¤íŒ¨: {e}")
                        logger.error(f"Sample data insertion error: {e}")
        
        with col2:
            if st.button("ë¬¸ì„œ ë°ì´í„° ì‚­ì œ"):
                document_id = st.text_input("ì‚­ì œí•  ë¬¸ì„œ ID:", "ai_handbook")
                if st.button("ì‚­ì œ ì‹¤í–‰", type="secondary"):
                    with st.spinner("ë°ì´í„° ì‚­ì œ ì¤‘..."):
                        try:
                            success = vector_store.delete_document_chunks(document_id)
                            if success:
                                st.success(f"âœ… ë¬¸ì„œ '{document_id}'ì˜ ì²­í¬ë“¤ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                            else:
                                st.error("âŒ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨")
                        except Exception as e:
                            st.error(f"ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        # Document chunks viewer
        st.subheader("ğŸ“„ ë¬¸ì„œ ì²­í¬ ì¡°íšŒ")
        doc_id_input = st.text_input("ë¬¸ì„œ ID ì…ë ¥:", "ai_handbook")
        
        if st.button("ì²­í¬ ì¡°íšŒ"):
            with st.spinner("ì²­í¬ ì¡°íšŒ ì¤‘..."):
                try:
                    chunks = vector_store.get_document_chunks(doc_id_input)
                    
                    if chunks:
                        st.success(f"âœ… {len(chunks)}ê°œì˜ ì²­í¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                        
                        for i, chunk in enumerate(chunks, 1):
                            with st.expander(f"ì²­í¬ {i}: {chunk['chunk_id']} (í˜ì´ì§€ {chunk.get('page_number', '?')})"):
                                st.write("**ë‚´ìš©:**")
                                st.write(chunk["content"])
                                st.write("**í‚¤ì›Œë“œ:**", ", ".join(chunk.get("keywords", [])))
                                st.write("**ì¤‘ìš”ë„:**", chunk.get("importance_score", 0))
                    else:
                        st.warning("í•´ë‹¹ ë¬¸ì„œì˜ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                except Exception as e:
                    st.error(f"ì²­í¬ ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    with tab5:
        st.header("ğŸ“š ë°ì´í„° ê´€ë¦¬")
        
    with tab6:
        st.header("ğŸ“Š ë¶„ì„ ë° í†µê³„")
        
        # Collection statistics
        st.subheader("ğŸ“Š ì»¬ë ‰ì…˜ í†µê³„")
        
        try:
            stats = vector_store.get_collection_stats()
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì—”í‹°í‹° ìˆ˜", stats.get("total_entities", 0))
            with col2:
                st.metric("ì¸ë±ìŠ¤ ìƒíƒœ", stats.get("index_status", "unknown"))
            with col3:
                st.metric("ì»¬ë ‰ì…˜ëª…", stats.get("collection_name", ""))
            
        except Exception as e:
            st.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # Cost estimation
        st.subheader("ğŸ’° ë¹„ìš© ì¶”ì •")
        
        col1, col2 = st.columns(2)
        with col1:
            text_count = st.number_input("í…ìŠ¤íŠ¸ ìˆ˜", min_value=1, value=100)
        with col2:
            avg_tokens = st.number_input("í‰ê·  í† í° ìˆ˜", min_value=1, value=100)
        
        if st.button("ë¹„ìš© ê³„ì‚°"):
            cost_info = embedding_service.estimate_cost(text_count, avg_tokens)
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ í† í° ìˆ˜", f"{cost_info['estimated_total_tokens']:,}")
            with col2:
                st.metric("ì˜ˆìƒ ë¹„ìš© (USD)", f"${cost_info['estimated_cost_usd']:.6f}")
            with col3:
                st.metric("ëª¨ë¸", cost_info['model'])
    
    with tab7:
        st.header("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
        
        st.subheader("ğŸ—ï¸ ì•„í‚¤í…ì²˜")
        st.info("""
        **PDF RAG ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ:**
        
        1. **ì„ë² ë”© ì„œë¹„ìŠ¤**: OpenAI text-embedding-ada-002 ëª¨ë¸ ì‚¬ìš©
        2. **ë²¡í„° ì €ì¥ì†Œ**: Milvus í´ë¼ìš°ë“œ (Zilliz) ì‚¬ìš©
        3. **ê²€ìƒ‰ ì—ì´ì „íŠ¸**: ì‹œë©˜í‹± ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        4. **ì§ˆë¬¸ ì²˜ë¦¬**: ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜• ì§€ì›
        5. **ë‹µë³€ ìƒì„±**: êµ¬ì¡°í™”ëœ ë‹µë³€ ë° ì¶œì²˜ ì •ë³´ ì œê³µ
        """)
        
        st.subheader("ğŸ”§ ê¸°ëŠ¥")
        st.success("""
        **í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
        
        âœ… ë²¡í„° DB ì—°ê²° ë° ê´€ë¦¬  
        âœ… ë¬¸ì„œ ì²­í¬ ì„ë² ë”© ë° ì €ì¥  
        âœ… ì‹œë©˜í‹± ìœ ì‚¬ë„ ê²€ìƒ‰  
        âœ… ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬  
        âœ… ë‹µë³€ ìƒì„± ë° ì‹ ë¢°ë„ í‰ê°€  
        âœ… ì†ŒìŠ¤ ì°¸ì¡° ë° ë©”íƒ€ë°ì´í„°  
        âœ… ì›¹ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
        âœ… LangChain/LangGraph ê¸°ë°˜ ì‹œìŠ¤í…œ  
        """)
        
        st.subheader("ğŸš€ LangGraph ì‹œìŠ¤í…œ íŠ¹ì§•")
        st.success("""
        **ìƒˆë¡œ ì¶”ê°€ëœ LangGraph ê¸°ëŠ¥:**
        
        âœ… ì§€ëŠ¥ì  ì›Œí¬í”Œë¡œìš° ê¸°ë°˜ ë¼ìš°íŒ…  
        âœ… ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ ì‹¤í–‰  
        âœ… ì¡°ê±´ë¶€ ë…¸ë“œ ì‹¤í–‰  
        âœ… ë‚´ì¥ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§  
        âœ… í‘œì¤€í™”ëœ ì—ì´ì „íŠ¸ ì¸í„°í˜ì´ìŠ¤  
        âœ… í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜  
        """)
        
        st.subheader("ğŸ“ ì‚¬ìš©ë²•")
        st.markdown("""
        1. **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**: `.env` íŒŒì¼ì— OpenAI API í‚¤ì™€ Milvus ì •ë³´ ì„¤ì •
        2. **ì§ˆë¬¸ ì…ë ¥**: ìì—°ì–´ë¡œ ì§ˆë¬¸ ì‘ì„±
        3. **ì‹œìŠ¤í…œ ì„ íƒ**: ê¸°ì¡´ ì‹œìŠ¤í…œ ë˜ëŠ” LangGraph ì‹œìŠ¤í…œ ì„ íƒ
        4. **ë‹µë³€ í™•ì¸**: ìƒì„±ëœ ë‹µë³€ê³¼ ë©”íƒ€ë°ì´í„° ê²€í† 
        5. **ì„±ëŠ¥ ë¶„ì„**: ì²˜ë¦¬ ì‹œê°„ ë° ì‹ ë¢°ë„ ë¶„ì„
        """)
    
    with tab3:
        st.header("ğŸš€ LangGraph ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ")
        
        st.info("""
        **ìƒˆë¡œìš´ LangChain/LangGraph ê¸°ë°˜ ì‹œìŠ¤í…œ**
        
        ì´ íƒ­ì—ì„œëŠ” LangChainê³¼ LangGraphë¥¼ ì‚¬ìš©í•œ ìƒˆë¡œìš´ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë¹„êµí•˜ì—¬ ë” ì§€ëŠ¥ì ì¸ ë¼ìš°íŒ…ê³¼ í†µí•©ëœ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
        """)
        
        # Initialize LangChain service (cached)
        @st.cache_resource
        def get_langchain_service():
            try:
                from src.langchain.services.langchain_answer_service import LangChainAnswerService
                return LangChainAnswerService(
                    vector_store=vector_store,
                    openai_api_key=config.openai_api_key,
                    vector_db_threshold=0.7,
                    web_search_threshold=0.6,
                    llm_direct_threshold=0.5
                )
            except Exception as e:
                st.error(f"LangChain ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return None
        
        langchain_service = get_langchain_service()
        
        if langchain_service:
            # Question input
            question_input = st.text_area(
                "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="ì˜ˆ: 2024ë…„ ìµœì‹  AI ê¸°ìˆ ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•˜ê³  í™œìš©ì‚¬ë¡€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”",
                height=100,
                key="langchain_question"
            )
            
            col1, col2, col3 = st.columns([2, 1, 1])
            
            with col1:
                if st.button("ğŸš€ LangGraphë¡œ ë‹µë³€ ë°›ê¸°", type="primary", key="langchain_submit"):
                    if question_input.strip():
                        with st.spinner("LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘..."):
                            try:
                                # Create question object
                                question = Question(content=question_input.strip())
                                
                                # Get comprehensive answer
                                result = langchain_service.get_comprehensive_answer(question)
                                
                                if result.get('success'):
                                    st.success("âœ… LangGraph ë‹µë³€ ì™„ë£Œ!")
                                    
                                    # Display answer
                                    answer = result.get('answer')
                                    if answer:
                                        st.subheader("ğŸ“ ë‹µë³€")
                                        st.markdown(answer.content)
                                        
                                        # Show routing info
                                        st.subheader("ğŸ§  ë¼ìš°íŒ… ì •ë³´")
                                        routing_info = result.get('routing_info', {})
                                        
                                        col1, col2, col3 = st.columns(3)
                                        with col1:
                                            st.metric("ë¼ìš°íŒ… ì „ëµ", routing_info.get('strategy', 'N/A'))
                                        with col2:
                                            st.metric("ì§ˆë¬¸ ìœ í˜•", str(routing_info.get('question_type', 'N/A')))
                                        with col3:
                                            st.metric("ì²˜ë¦¬ ì‹œê°„", f"{routing_info.get('total_processing_time', 0):.0f}ms")
                                        
                                        # Show confidence scores
                                        confidences = routing_info.get('source_confidences', {})
                                        if confidences:
                                            st.subheader("ğŸ“Š ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„")
                                            
                                            # Create confidence chart
                                            sources = list(confidences.keys())
                                            scores = [confidences[s] * 100 for s in sources]
                                            
                                            fig = go.Figure(data=[
                                                go.Bar(
                                                    x=sources,
                                                    y=scores,
                                                    marker_color=['#4CAF50', '#2196F3', '#FF9800'][:len(sources)]
                                                )
                                            ])
                                            fig.update_layout(
                                                title="ì†ŒìŠ¤ë³„ ì‹ ë¢°ë„ ì ìˆ˜",
                                                yaxis_title="ì‹ ë¢°ë„ (%)",
                                                xaxis_title="ë‹µë³€ ì†ŒìŠ¤",
                                                height=400
                                            )
                                            st.plotly_chart(fig, use_container_width=True)
                                        
                                        # Show performance metrics
                                        st.subheader("âš¡ ì„±ëŠ¥ ì§€í‘œ")
                                        perf_metrics = result.get('performance_metrics', {})
                                        agent_times = perf_metrics.get('agent_times', {})
                                        
                                        if agent_times:
                                            df = pd.DataFrame([
                                                {"ì—ì´ì „íŠ¸": k.replace('_', ' ').title(), "ì²˜ë¦¬ì‹œê°„ (ms)": v}
                                                for k, v in agent_times.items()
                                            ])
                                            st.dataframe(df, use_container_width=True)
                                        
                                        # Show metadata
                                        with st.expander("ğŸ” ìƒì„¸ ë©”íƒ€ë°ì´í„°"):
                                            st.json(result.get('metadata', {}))
                                            
                                else:
                                    st.error(f"âŒ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                                    
                            except Exception as e:
                                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                                logger.error(f"LangGraph error: {e}")
                                st.code(traceback.format_exc())
                    else:
                        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            
            with col2:
                if st.button("ğŸ“Š ì„œë¹„ìŠ¤ í†µê³„", key="langchain_stats"):
                    stats = langchain_service.get_service_stats()
                    st.json(stats)
            
            with col3:
                if st.button("ğŸ”„ í†µê³„ ë¦¬ì…‹", key="langchain_reset"):
                    langchain_service.reset_stats()
                    st.success("í†µê³„ê°€ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # System comparison section
            st.subheader("âš–ï¸ ì‹œìŠ¤í…œ ë¹„êµ")
            
            with st.expander("ê¸°ì¡´ ì‹œìŠ¤í…œ vs LangGraph ì‹œìŠ¤í…œ"):
                comparison_df = pd.DataFrame({
                    "íŠ¹ì§•": [
                        "ì•„í‚¤í…ì²˜", 
                        "ë¼ìš°íŒ…", 
                        "ìƒíƒœ ê´€ë¦¬", 
                        "í™•ì¥ì„±", 
                        "ë””ë²„ê¹…", 
                        "í‘œì¤€í™”"
                    ],
                    "ê¸°ì¡´ ì‹œìŠ¤í…œ": [
                        "ì»¤ìŠ¤í…€ Python í´ë˜ìŠ¤",
                        "ìˆ˜ë™ íŒ¨í„´ ë§¤ì¹­",
                        "ë”•ì…”ë„ˆë¦¬ ê¸°ë°˜",
                        "ì œí•œì ",
                        "ìˆ˜ë™ ë¡œê¹…",
                        "ì»¤ìŠ¤í…€ êµ¬í˜„"
                    ],
                    "LangGraph ì‹œìŠ¤í…œ": [
                        "LangChain/LangGraph",
                        "ì§€ëŠ¥ì  ì›Œí¬í”Œë¡œìš°",
                        "TypedDict ìƒíƒœ ê´€ë¦¬",
                        "ë†’ìŒ (ë…¸ë“œ ì¶”ê°€ ìš©ì´)",
                        "ë‚´ì¥ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§",
                        "ì—…ê³„ í‘œì¤€ í”„ë ˆì„ì›Œí¬"
                    ]
                })
                st.dataframe(comparison_df, use_container_width=True)
        
        else:
            st.error("LangChain ì„œë¹„ìŠ¤ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    with tab4:
        st.header("ì§ˆë¬¸ ë¼ìš°í„°")
        
        st.subheader("ğŸ—ï¸ ì•„í‚¤í…ì²˜")
        st.info("""
        **PDF RAG ì‹œìŠ¤í…œ êµ¬ì„±ìš”ì†Œ:**
        
        1. **ì„ë² ë”© ì„œë¹„ìŠ¤**: OpenAI text-embedding-ada-002 ëª¨ë¸ ì‚¬ìš©
        2. **ë²¡í„° ì €ì¥ì†Œ**: Milvus í´ë¼ìš°ë“œ (Zilliz) ì‚¬ìš©
        3. **ê²€ìƒ‰ ì—ì´ì „íŠ¸**: ì‹œë©˜í‹± ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        4. **ì§ˆë¬¸ ì²˜ë¦¬**: ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜• ì§€ì›
        5. **ë‹µë³€ ìƒì„±**: êµ¬ì¡°í™”ëœ ë‹µë³€ ë° ì¶œì²˜ ì •ë³´ ì œê³µ
        """)
        
        st.subheader("ğŸ”§ ê¸°ëŠ¥")
        st.success("""
        **í˜„ì¬ êµ¬í˜„ëœ ê¸°ëŠ¥:**
        
        âœ… ë²¡í„° DB ì—°ê²° ë° ê´€ë¦¬  
        âœ… ë¬¸ì„œ ì²­í¬ ì„ë² ë”© ë° ì €ì¥  
        âœ… ì‹œë©˜í‹± ìœ ì‚¬ë„ ê²€ìƒ‰  
        âœ… ì§ˆë¬¸ ìœ í˜•ë³„ ì²˜ë¦¬  
        âœ… ë‹µë³€ ìƒì„± ë° ì‹ ë¢°ë„ í‰ê°€  
        âœ… ì†ŒìŠ¤ ì°¸ì¡° ë° ë©”íƒ€ë°ì´í„°  
        âœ… ì›¹ ì¸í„°í˜ì´ìŠ¤ ì œê³µ  
        """)
        
        st.subheader("ğŸš€ í–¥í›„ ê³„íš")
        st.warning("""
        **êµ¬í˜„ ì˜ˆì • ê¸°ëŠ¥:**
        
        ğŸ”„ ì§ˆë¬¸ ë¼ìš°í„° (ë‹¤ì¤‘ ì†ŒìŠ¤ ê²°ì •)  
        ğŸ”„ ì›¹ ê²€ìƒ‰ ì—°ë™  
        ğŸ”„ LLM ì§ì ‘ ë‹µë³€  
        ğŸ”„ ë‹µë³€ í†µí•© ì„œë¹„ìŠ¤  
        ğŸ”„ ì‚¬ìš©ì í”¼ë“œë°± ì‹œìŠ¤í…œ  
        """)
        
        st.subheader("ğŸ“ ì‚¬ìš©ë²•")
        st.markdown("""
        1. **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**: `.env` íŒŒì¼ì— OpenAI API í‚¤ì™€ Milvus ì •ë³´ ì„¤ì •
        2. **ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€**: 'ë°ì´í„° ê´€ë¦¬' íƒ­ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ì¶”ê°€
        3. **ì§ˆë¬¸ ì…ë ¥**: 'ì§ˆì˜ì‘ë‹µ' íƒ­ì—ì„œ ì§ˆë¬¸ ì…ë ¥ ë° ê²€ìƒ‰
        4. **ê²°ê³¼ í™•ì¸**: ë‹µë³€, ìœ ì‚¬ë„ ì ìˆ˜, ì°¸ì¡° ì†ŒìŠ¤ í™•ì¸
        """)


if __name__ == "__main__":
    main()